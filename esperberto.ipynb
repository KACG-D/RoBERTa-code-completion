{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a1b01e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n",
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-azokl1jo\n",
      "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-azokl1jo\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/sdlab/.local/lib/python3.8/site-packages (from transformers==4.12.0.dev0) (1.21.2)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers==4.12.0.dev0) (2.22.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/sdlab/.local/lib/python3.8/site-packages (from transformers==4.12.0.dev0) (0.10.3)\n",
      "Requirement already satisfied: filelock in /home/sdlab/.local/lib/python3.8/site-packages (from transformers==4.12.0.dev0) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sdlab/.local/lib/python3.8/site-packages (from transformers==4.12.0.dev0) (21.0)\n",
      "Requirement already satisfied: sacremoses in /home/sdlab/.local/lib/python3.8/site-packages (from transformers==4.12.0.dev0) (0.0.46)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers==4.12.0.dev0) (5.3.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in /home/sdlab/.local/lib/python3.8/site-packages (from transformers==4.12.0.dev0) (0.0.19)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sdlab/.local/lib/python3.8/site-packages (from transformers==4.12.0.dev0) (2021.10.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/sdlab/.local/lib/python3.8/site-packages (from transformers==4.12.0.dev0) (4.62.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/sdlab/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.12.0.dev0) (2.4.7)\n",
      "Requirement already satisfied: joblib in /home/sdlab/.local/lib/python3.8/site-packages (from sacremoses->transformers==4.12.0.dev0) (1.1.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from sacremoses->transformers==4.12.0.dev0) (1.14.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from sacremoses->transformers==4.12.0.dev0) (7.0)\n",
      "Requirement already satisfied: typing-extensions in /home/sdlab/.local/lib/python3.8/site-packages (from huggingface-hub>=0.0.17->transformers==4.12.0.dev0) (3.10.0.2)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.12.0.dev0-py3-none-any.whl size=2943553 sha256=9974030dd52d7227e736f3101fae88d0e2d8816ee66e287e12618a87b48ae7ea\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-erfqdzdx/wheels/42/68/45/c63edff61c292f2dfd4df4ef6522dcbecc603e7af82813c1d7\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.11.3\n",
      "    Uninstalling transformers-4.11.3:\n",
      "      Successfully uninstalled transformers-4.11.3\n",
      "Successfully installed transformers-4.12.0.dev0\n",
      "tokenizers              0.10.3              \n",
      "transformers            4.12.0.dev0         \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3890ae57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./tokenizer_training']\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 1min 44s, sys: 2.1 s, total: 1min 46s\n",
      "Wall time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# „Éë„Çπ ['oscar.eo.txt']\n",
    "paths = ['./tokenizer_training']\n",
    "print(paths)\n",
    "# „Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„ÅÆÂàùÊúüÂåñ\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Â≠¶Áøí\n",
    "tokenizer.train(files=paths, vocab_size=5000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "096d49cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "mkdir: „Éá„Ç£„É¨„ÇØ„Éà„É™ `EsperBERTo' „Çí‰ΩúÊàê„Åß„Åç„Åæ„Åõ„Çì: „Éï„Ç°„Ç§„É´„ÅåÂ≠òÂú®„Åó„Åæ„Åô\r\n"
     ]
    }
   ],
   "source": [
    "# „É¢„Éá„É´„ÅÆ‰øùÂ≠ò\n",
    "!mkdir EsperBERTo\n",
    "tokenizer.save_model(\"EsperBERTo\")\n",
    "tokenizer.save(\"EsperBERTo/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af37e8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "# „Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„ÅÆÊ∫ñÂÇô\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./EsperBERTo/vocab.json\",\n",
    "    \"./EsperBERTo/merges.txt\",\n",
    ")\n",
    "\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "   (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "   (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "474c3313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=97, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# „Ç®„É≥„Ç≥„Éº„Éâ\n",
    "tokenizer.encode(\"public void updateLockdownExceptions(ManagedObjectReference _this, String[] users) throws AuthMinimumAdminPermission, RemoteException, RuntimeFault, UserNotFound {\\n    Argument[] params = new Argument[2];\\n    params[0] = new Argument(\\\"_this\\\", \\\"ManagedObjectReference\\\", _this);\\n    params[1] = new Argument(\\\"users\\\", \\\"String[]\\\", users);\\n   \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab618db1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'public',\n",
       " 'ƒ†void',\n",
       " 'ƒ†update',\n",
       " 'Lock',\n",
       " 'down',\n",
       " 'Exception',\n",
       " 's',\n",
       " '(',\n",
       " 'Managed',\n",
       " 'Object',\n",
       " 'Reference',\n",
       " 'ƒ†_',\n",
       " 'this',\n",
       " ',',\n",
       " 'ƒ†String',\n",
       " '[]',\n",
       " 'ƒ†user',\n",
       " 's',\n",
       " ')',\n",
       " 'ƒ†throws',\n",
       " 'ƒ†Auth',\n",
       " 'Min',\n",
       " 'imum',\n",
       " 'Admin',\n",
       " 'Permission',\n",
       " ',',\n",
       " 'ƒ†Remote',\n",
       " 'Exception',\n",
       " ',',\n",
       " 'ƒ†Runtime',\n",
       " 'F',\n",
       " 'ault',\n",
       " ',',\n",
       " 'ƒ†User',\n",
       " 'NotFound',\n",
       " 'ƒ†{',\n",
       " 'ƒä',\n",
       " 'ƒ†ƒ†ƒ†',\n",
       " 'ƒ†',\n",
       " 'Argument',\n",
       " '[]',\n",
       " 'ƒ†params',\n",
       " 'ƒ†=',\n",
       " 'ƒ†new',\n",
       " 'ƒ†',\n",
       " 'Argument',\n",
       " '[',\n",
       " '2',\n",
       " ']',\n",
       " ';',\n",
       " 'ƒä',\n",
       " 'ƒ†ƒ†ƒ†',\n",
       " 'ƒ†params',\n",
       " '[',\n",
       " '0',\n",
       " ']',\n",
       " 'ƒ†=',\n",
       " 'ƒ†new',\n",
       " 'ƒ†',\n",
       " 'Argument',\n",
       " '(\"',\n",
       " '_',\n",
       " 'this',\n",
       " '\",',\n",
       " 'ƒ†\"',\n",
       " 'Managed',\n",
       " 'Object',\n",
       " 'Reference',\n",
       " '\",',\n",
       " 'ƒ†_',\n",
       " 'this',\n",
       " ');',\n",
       " 'ƒä',\n",
       " 'ƒ†ƒ†ƒ†',\n",
       " 'ƒ†params',\n",
       " '[',\n",
       " '1',\n",
       " ']',\n",
       " 'ƒ†=',\n",
       " 'ƒ†new',\n",
       " 'ƒ†',\n",
       " 'Argument',\n",
       " '(\"',\n",
       " 'user',\n",
       " 's',\n",
       " '\",',\n",
       " 'ƒ†\"',\n",
       " 'String',\n",
       " '[]',\n",
       " '\",',\n",
       " 'ƒ†user',\n",
       " 's',\n",
       " ');',\n",
       " 'ƒä',\n",
       " 'ƒ†ƒ†ƒ†',\n",
       " '</s>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# „Éà„Éº„ÇØ„É≥Âåñ\n",
    "tokenizer.encode(\"public void updateLockdownExceptions(ManagedObjectReference _this, String[] users) throws AuthMinimumAdminPermission, RemoteException, RuntimeFault, UserNotFound {\\n    Argument[] params = new Argument[2];\\n    params[0] = new Argument(\\\"_this\\\", \\\"ManagedObjectReference\\\", _this);\\n    params[1] = new Argument(\\\"users\\\", \\\"String[]\\\", users);\\n   \").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c7f216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov  4 18:23:05 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:09:00.0  On |                  N/A |\r\n",
      "|  0%   52C    P8    22W / 220W |    610MiB /  7982MiB |      4%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A       981      G   /usr/lib/xorg/Xorg                332MiB |\r\n",
      "|    0   N/A  N/A      1213      G   /usr/bin/gnome-shell               68MiB |\r\n",
      "|    0   N/A  N/A    119932      G   /usr/lib/firefox/firefox          192MiB |\r\n",
      "|    0   N/A  N/A    120250      G   /usr/lib/firefox/firefox            3MiB |\r\n",
      "|    0   N/A  N/A    120348      G   /usr/lib/firefox/firefox            3MiB |\r\n",
      "|    0   N/A  N/A    120580      G   /usr/lib/firefox/firefox            3MiB |\r\n",
      "|    0   N/A  N/A    120825      G   /usr/lib/firefox/firefox            3MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# GPU„ÅÆ„ÉÅ„Çß„ÉÉ„ÇØ\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe0a9123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch„ÅåGPU„ÇíË¶ã„Å¶„ÅÑ„Çã„Åì„Å®„ÇíÁ¢∫Ë™ç\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06a70356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "# „É¢„Éá„É´„ÅÆË®≠ÂÆö\n",
    "config = RobertaConfig(\n",
    "    model_type=\"roberta\",\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.3,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    hidden_size=768,\n",
    "    initalizer_range=0.2,\n",
    "    vocab_size=5000,\n",
    "    max_position_embeddings=1024,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=12,\n",
    "    type_vocab_size=1,\n",
    "    intermediate_size=4096,\n",
    "    cache_dir=\"./cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38a50bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "# „Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„ÅÆ‰ΩúÊàê\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./EsperBERTo\", max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84b8aa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "# „É¢„Éá„É´„ÅÆ‰ΩúÊàê\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c649b9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109166984"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57f894a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdlab/.local/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py:120: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 30s, sys: 1.87 s, total: 1min 32s\n",
      "Wall time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "# „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ‰ΩúÊàê\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./tokenizer_training\",\n",
    "    block_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "482285c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# data_collator„ÅÆ‰ΩúÊàê\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c820c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3995950\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    RobertaConfig,\n",
    "    RobertaForMaskedLM,\n",
    "    RobertaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "t_total = len(dataset) // 4 * 50\n",
    "print(t_total)\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=t_total\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f94683f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# „Éà„É¨„Éº„Éä„Éº„ÅÆÂºïÊï∞„ÅÆ‰ΩúÊàê\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./EsperBERTo\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size= 16,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps = 4\n",
    ")\n",
    "\n",
    "# „Éà„É¨„Éº„Éä„Éº„ÅÆ‰ΩúÊàê\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    optimizers = (optimizer,scheduler)\n",
    "    #prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f5809cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 319676\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 74925\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74925' max='74925' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74925/74925 10:44:07, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.969500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.430500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.087000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.555800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.082100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.717800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.432100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.194900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.989000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.778000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.620600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.472400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.340500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.227100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.123500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.931800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.852600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.799500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.732500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.671900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.623700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.579900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.543700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.506000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.468000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.446400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.417500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.394800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.366400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.345500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.317300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>1.302700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.278700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>1.257200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.244600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>1.225900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.204400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>1.205900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.189900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>1.170400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>1.150500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>1.141300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>1.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>1.114200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>1.103700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>1.085200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>1.078900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>1.075800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>1.063100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>1.055100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>1.048300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>1.031100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>1.019300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>1.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>1.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>1.008700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.993700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.993900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.979400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.975600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.953700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.953600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.936100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.939800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.945100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.933700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.921100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.918800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.913800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.912100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.907500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.904100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.902700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.894200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.883300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.882100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.866700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.879600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.872700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.872500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.864700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.854100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.858700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.855300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.843900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.844400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>0.837900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.836300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>0.832500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.837900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.821500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.826000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>0.823800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.814900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>0.810900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.812600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>0.802300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.804800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>0.804000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.800600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>0.797700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.793800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>0.795900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.792200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>0.787900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.785100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>0.787800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.779300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>0.774700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.775600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>0.777300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.766900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>0.768800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.767000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>0.763700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.764600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>0.755400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.753800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>0.742600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.743200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>0.751100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.749600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>0.745800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.733900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>0.744900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.743800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>0.738500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.734400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>0.729600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.727900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>0.728300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.724600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>0.723900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.717900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>0.724600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>0.714800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69500</td>\n",
       "      <td>0.713000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.712500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70500</td>\n",
       "      <td>0.700900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>0.709500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71500</td>\n",
       "      <td>0.698100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.703200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>0.714800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>0.711300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73500</td>\n",
       "      <td>0.701500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.699100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74500</td>\n",
       "      <td>0.698700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./EsperBERTo/checkpoint-10000\n",
      "Configuration saved in ./EsperBERTo/checkpoint-10000/config.json\n",
      "Model weights saved in ./EsperBERTo/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [EsperBERTo/checkpoint-60000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./EsperBERTo/checkpoint-20000\n",
      "Configuration saved in ./EsperBERTo/checkpoint-20000/config.json\n",
      "Model weights saved in ./EsperBERTo/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [EsperBERTo/checkpoint-70000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./EsperBERTo/checkpoint-30000\n",
      "Configuration saved in ./EsperBERTo/checkpoint-30000/config.json\n",
      "Model weights saved in ./EsperBERTo/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [EsperBERTo/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./EsperBERTo/checkpoint-40000\n",
      "Configuration saved in ./EsperBERTo/checkpoint-40000/config.json\n",
      "Model weights saved in ./EsperBERTo/checkpoint-40000/pytorch_model.bin\n",
      "Deleting older checkpoint [EsperBERTo/checkpoint-20000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./EsperBERTo/checkpoint-50000\n",
      "Configuration saved in ./EsperBERTo/checkpoint-50000/config.json\n",
      "Model weights saved in ./EsperBERTo/checkpoint-50000/pytorch_model.bin\n",
      "Deleting older checkpoint [EsperBERTo/checkpoint-30000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./EsperBERTo/checkpoint-60000\n",
      "Configuration saved in ./EsperBERTo/checkpoint-60000/config.json\n",
      "Model weights saved in ./EsperBERTo/checkpoint-60000/pytorch_model.bin\n",
      "Deleting older checkpoint [EsperBERTo/checkpoint-40000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./EsperBERTo/checkpoint-70000\n",
      "Configuration saved in ./EsperBERTo/checkpoint-70000/config.json\n",
      "Model weights saved in ./EsperBERTo/checkpoint-70000/pytorch_model.bin\n",
      "Deleting older checkpoint [EsperBERTo/checkpoint-50000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=74925, training_loss=1.2245778371144582, metrics={'train_runtime': 38647.9211, 'train_samples_per_second': 124.072, 'train_steps_per_second': 1.939, 'total_flos': 3.835502302343274e+17, 'train_loss': 1.2245778371144582, 'epoch': 15.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a3a1257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./EsperBERTo\n",
      "Configuration saved in ./EsperBERTo/config.json\n",
      "Model weights saved in ./EsperBERTo/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./EsperBERTo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d8e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# fill-mask„Éë„Ç§„Éó„É©„Ç§„É≥„ÅÆ‰ΩúÊàê\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./EsperBERTo\",\n",
    "    tokenizer=\"./EsperBERTo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d361a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'int getValue(){ return this.value;}',\n",
       "  'score': 0.8728135824203491,\n",
       "  'token': 301,\n",
       "  'token_str': ' return'},\n",
       " {'sequence': 'int getValue(){return this.value;}',\n",
       "  'score': 0.12403827160596848,\n",
       "  'token': 3789,\n",
       "  'token_str': 'return'},\n",
       " {'sequence': 'int getValue(){ throw this.value;}',\n",
       "  'score': 0.0012877093395218253,\n",
       "  'token': 383,\n",
       "  'token_str': ' throw'},\n",
       " {'sequence': 'int getValue(){= this.value;}',\n",
       "  'score': 0.0008061031112447381,\n",
       "  'token': 33,\n",
       "  'token_str': '='},\n",
       " {'sequence': 'int getValue(){  this.value;}',\n",
       "  'score': 0.000371721776900813,\n",
       "  'token': 225,\n",
       "  'token_str': ' '}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The sun <mask>.\n",
    "fill_mask(\"int getValue(){<mask> this.value;}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6039f4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'String getValue(){return this.value;}',\n",
       "  'score': 0.6368172764778137,\n",
       "  'token': 309,\n",
       "  'token_str': 'String'},\n",
       " {'sequence': 'Object getValue(){return this.value;}',\n",
       "  'score': 0.14222966134548187,\n",
       "  'token': 412,\n",
       "  'token_str': 'Object'},\n",
       " {'sequence': 'T getValue(){return this.value;}',\n",
       "  'score': 0.03950953856110573,\n",
       "  'token': 56,\n",
       "  'token_str': 'T'},\n",
       " {'sequence': 'E getValue(){return this.value;}',\n",
       "  'score': 0.03333442658185959,\n",
       "  'token': 41,\n",
       "  'token_str': 'E'},\n",
       " {'sequence': 'int getValue(){return this.value;}',\n",
       "  'score': 0.024799060076475143,\n",
       "  'token': 439,\n",
       "  'token_str': 'int'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the beginning of a beautiful <mask>.\n",
    "fill_mask(\"<mask> getValue(){return this.value;}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa19b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09830b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3140b16d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b90bdf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd197b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f14380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7150ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cf5f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d272e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b6b96c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
